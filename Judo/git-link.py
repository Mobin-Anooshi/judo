# import requests
# import json
#
# base_url = "https://judotv.com/api/v2/competitions/"
# headers = {
#     "User-Agent": "Mozilla/5.0"
# }
#
# # Fetch competitions from 2014 to 2019
# date_from = "2014-01-01"
# date_to = "2025-12-31"
# page = 1
# all_competitions = []
#
# # Step 1: Fetch all competitions
# while True:
#     params = {
#         "DateFrom": date_from,
#         "DateTo": date_to,
#         "PerPage": 50,
#         "Page": page,
#         "SortingField": "date_from.asc"
#     }
#     print(f"ğŸ“„ Fetching competitions page {page}...")
#     response = requests.get(base_url, params=params, headers=headers)
#
#     if response.status_code != 200:
#         print(f"âŒ Failed to fetch page {page}: Status code {response.status_code}")
#         break
#
#     try:
#         data = response.json()
#     except Exception as e:
#         print("âŒ Failed to parse JSON:", e)
#         break
#
#     competitions = data.get("list", [])
#     if not competitions:
#         print("âœ… No more competitions.")
#         break
#
#     all_competitions.extend(competitions)
#     page += 1
#
# # Step 2: Fetch match/draw data for each competition
# all_matches = []
# for comp in all_competitions:
#     external_id = comp["externalId"]
#     match_url = f"https://judotv.com/api/v2/competitions/{external_id}/draw"  # Hypothetical endpoint
#     print(f"ğŸ“„ Fetching matches for competition {external_id}...")
#
#     response = requests.get(match_url, headers=headers)
#
#     if response.status_code != 200:
#         print(f"âŒ Failed to fetch matches for {external_id}: Status code {response.status_code}")
#         continue
#
#     try:
#         match_data = response.json()
#         matches = match_data.get("matches", [])  # Adjust based on actual API structure
#         filtered_matches = [
#             {
#                 "competition_id": external_id,
#                 "match_id": match.get("id"),
#                 "players": match.get("players"),
#                 "result": match.get("result"),
#                 "category": match.get("category")
#             }
#             for match in matches
#         ]
#         all_matches.extend(filtered_matches)
#     except Exception as e:
#         print(f"âŒ Failed to parse matches for {external_id}: {e}")
#         continue
#
# # Step 3: Save matches to JSON
# with open("filtered_matches_2014_2025.json", "w") as f:
#     json.dump(all_matches, f, indent=2)
#
# print(f"âœ… Done. Total matches processed: {len(all_matches)}")
import requests
import json
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import time

base_url = "https://judotv.com/api/v2/competitions/"
draw_base_url = "https://judotv.com/competitions/{}/draw"
headers = {
    "User-Agent": "Mozilla/5.0"
}

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª selenium Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ±
chrome_options = Options()
chrome_options.add_argument("--headless")  # Ø¨Ø¯ÙˆÙ† Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† Ù…Ø±ÙˆØ±Ú¯Ø±
chrome_options.add_argument("--disable-gpu")
driver = webdriver.Chrome(options=chrome_options)

# Ú¯Ø±ÙØªÙ† Ù…Ø³Ø§Ø¨Ù‚Ø§Øª Ø§Ø² Û²Û°Û±Û´ ØªØ§ Û²Û°Û±Û¹
date_from = "2014-01-01"
date_to = "2019-12-31"
page = 1
all_competitions = []

while True:
    params = {
        "DateFrom": date_from,
        "DateTo": date_to,
        "PerPage": 50,
        "Page": page,
        "SortingField": "date_from.asc"
    }
    print(f"ğŸ“„ Ø¯Ø± Ø­Ø§Ù„ Ú¯Ø±ÙØªÙ† ØµÙØ­Ù‡ {page} Ø§Ø² Ù…Ø³Ø§Ø¨Ù‚Ø§Øª...")
    response = requests.get(base_url, params=params, headers=headers)

    if response.status_code != 200:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ú¯Ø±ÙØªÙ† ØµÙØ­Ù‡ {page}: Ú©Ø¯ ÙˆØ¶Ø¹ÛŒØª {response.status_code}")
        break

    try:
        data = response.json()
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ¬Ø²ÛŒÙ‡ JSON: {e}")
        break

    competitions = data.get("list", [])
    if not competitions:
        print("âœ… Ø¯ÛŒÚ¯Ù‡ Ù…Ø³Ø§Ø¨Ù‚Ù‡â€ŒØ§ÛŒ Ù†ÛŒØ³Øª.")
        break

    all_competitions.extend(competitions)
    page += 1

# Ú¯Ø±ÙØªÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ draw Ø¨Ø§ selenium
all_matches = []
for comp in all_competitions[:5]:  # ÙÙ‚Ø· Ûµ Ù…Ø³Ø§Ø¨Ù‚Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ³Øª
    external_id = comp.get("externalId")
    if not external_id or external_id == "None":
        print(f"âŒ externalId Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø³Ø§Ø¨Ù‚Ù‡: {comp.get('name')}")
        continue

    draw_url = draw_base_url.format(external_id)
    print(f"ğŸ“„ Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³Ú©Ø±Ù¾ Ú©Ø±Ø¯Ù† draw Ø¨Ø±Ø§ÛŒ Ù…Ø³Ø§Ø¨Ù‚Ù‡ {external_id}...")

    try:
        driver.get(draw_url)
        time.sleep(2)  # ØµØ¨Ø± Ø¨Ø±Ø§ÛŒ Ù„ÙˆØ¯ Ú©Ø§Ù…Ù„ ØµÙØ­Ù‡
        soup = BeautifulSoup(driver.page_source, "html.parser")

        # ÙØ±Ø¶: Ø¨Ø§Ø²ÛŒâ€ŒÙ‡Ø§ ØªÙˆ ØªÚ¯ div Ø¨Ø§ Ú©Ù„Ø§Ø³ "match-item" (Ø¨Ø§ÛŒØ¯ Ø¨Ø§ HTML ÙˆØ§Ù‚Ø¹ÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø¨Ø´Ù‡)
        match_elements = soup.find_all("div", class_="match-item")  # Ú©Ù„Ø§Ø³ ÙˆØ§Ù‚Ø¹ÛŒ Ø±Ùˆ Ù¾ÛŒØ¯Ø§ Ú©Ù†
        matches = []
        for match in match_elements:
            match_data = {
                "competition_name": comp.get("name"),
                "competition_externalId": external_id,
                "dateFrom": comp.get("dateFrom"),
                "dateTo": comp.get("dateTo"),
                "match_id": match.get("data-id", "unknown"),
                "players": match.find("span", class_="players").text if match.find("span", class_="players") else "N/A",
                "result": match.find("span", class_="result").text if match.find("span", class_="result") else "N/A",
                "category": match.find("span", class_="category").text if match.find("span",
                                                                                     class_="category") else "N/A"
            }
            matches.append(match_data)
        all_matches.extend(matches)
        print(f"âœ… {len(matches)} Ø¨Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ {external_id} Ù¾ÛŒØ¯Ø§ Ø´Ø¯.")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³Ú©Ø±Ù¾ Ú©Ø±Ø¯Ù† draw Ø¨Ø±Ø§ÛŒ {external_id}: {e}")
        continue

    time.sleep(1)  # ÙØ§ØµÙ„Ù‡ Ø¨ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§

driver.quit()

# Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª JSON
with open("scraped_matches_2015 _2019.json", "w") as f:
    json.dump(all_matches, f, indent=2)

print(f"âœ… ØªÙ…ÙˆÙ… Ø´Ø¯. ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø¨Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡: {len(all_matches)}")